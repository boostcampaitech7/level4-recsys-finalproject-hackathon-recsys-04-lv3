{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Index 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import library\n",
    "from pinecone import Pinecone, ServerlessSpec\n",
    "from langchain_pinecone import PineconeVectorStore\n",
    "from langchain_upstage import ChatUpstage, UpstageEmbeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import PyMuPDFLoader\n",
    "from langchain_core.documents import Document\n",
    "from dotenv import load_dotenv\n",
    "import glob\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "\n",
    "index_name = \"dev-02\"\n",
    "pc = Pinecone()\n",
    "llm_upstage = ChatUpstage(api_key=os.environ.get(\"UPSTAGE_API_KEY\"), temperature=0, model=\"solar-pro\")\n",
    "embeddings_query = UpstageEmbeddings(model=\"embedding-query\") #4096"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dev-02 is already exists.\n"
     ]
    }
   ],
   "source": [
    "if index_name not in [index_info[\"name\"] for index_info in pc.list_indexes()]:\n",
    "    pc.create_index(\n",
    "        name=index_name,\n",
    "        dimension=4096, \n",
    "        metric=\"dotproduct\",\n",
    "        spec=ServerlessSpec(\n",
    "            cloud=\"aws\",\n",
    "            region=\"us-east-1\"\n",
    "        ) \n",
    "    )\n",
    "    print(f\"{index_name} has been successfully created\")\n",
    "else:\n",
    "    print(f\"{index_name} is already exists.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = pc.Index(index_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/venv/lib/python3.12/site-packages/langchain_community/document_loaders/parsers/pdf.py:322: UserWarning: Warning: Empty content on page 225 of document data/천재교육_고등교과서_지구과학Ⅱ_오필석(15개정)_교과서 본문.pdf\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1579"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm = ChatUpstage(api_key=os.environ.get(\"UPSTAGE_API_KEY\"), temperature=0, model=\"solar-pro\")\n",
    "embeddings_passage = UpstageEmbeddings(model=\"embedding-passage\") #4096\n",
    "embeddings_query = UpstageEmbeddings(model=\"embedding-query\") #4096\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=150)\n",
    "\n",
    "split_docs = []\n",
    "files = sorted(glob.glob(\"data/*.pdf\"))\n",
    "\n",
    "for file in files:\n",
    "    loader = PyMuPDFLoader(file)\n",
    "    split_docs.extend(loader.load_and_split(text_splitter))\n",
    "\n",
    "# 문서 개수 확인\n",
    "len(split_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def remove_question_and_specific_sentences(paragraph):\n",
    "    # 정규표현식으로 \"?\" 또는 \"보자\"로 끝나는 문장 제거\n",
    "    result = re.sub(r'[^.?!]*[\\?]|[^.?!]*보자\\.', '', paragraph)\n",
    "    # 여러 개의 공백을 하나로 정리\n",
    "    result = re.sub(r'\\s+', ' ', result).strip()\n",
    "    return result\n",
    "\n",
    "for doc in split_docs:\n",
    "    doc.page_content = remove_question_and_specific_sentences(doc.page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docsearch = PineconeVectorStore.from_existing_index(index_name=index_name, embedding=embeddings_passage)\n",
    "# docsearch.add_documents(split_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 테스트 데이터 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "832"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_docs = []\n",
    "files = sorted(glob.glob(\"test_data/*.pdf\"))\n",
    "\n",
    "for file in files:\n",
    "    loader = PyMuPDFLoader(file)\n",
    "    split_docs.extend(loader.load_and_split(text_splitter))\n",
    "\n",
    "# 문서 개수 확인\n",
    "len(split_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "총 171개의 해설을 추출하여 output.txt에 저장했습니다.\n"
     ]
    }
   ],
   "source": [
    "### 지구과학 2\n",
    "from langchain_community.document_loaders import PyMuPDFLoader\n",
    "import re\n",
    "\n",
    "def extract_explanations(pdf_path, output_path):\n",
    "    \"\"\"\n",
    "    PDF에서 \"해설 | \"로 시작하고 마침표로 끝나는 문장을 추출하여 파일로 저장하는 함수\n",
    "    \n",
    "    Args:\n",
    "        pdf_path (str): 입력 PDF 파일 경로\n",
    "        output_path (str): 출력 텍스트 파일 경로\n",
    "    \"\"\"\n",
    "    # PDF 로드\n",
    "    loader = PyMuPDFLoader(pdf_path)\n",
    "    pages = loader.load()\n",
    "    start_word = \"해설\"\n",
    "    end_word = \"ㄱ\"\n",
    "    pattern = rf\"{start_word}.*?{end_word}\\.\"\n",
    "    matches = []\n",
    "    for page in pages:\n",
    "        matches.extend(re.findall(pattern, page.page_content, re.DOTALL))\n",
    "        \n",
    "    matches = [re.sub(rf\"\\s*{end_word}\\.$\", \"\", match) for match in matches]\n",
    "    matches = [s[s.index(start_word):] for s in matches]\n",
    "    \n",
    "    hint_patterns = [r\"\\d\", r\"㉠\", r\"[a-zA-Z]\", r\"\\(가\\)\", r\"\\(나\\)\"]\n",
    "    explanations = []\n",
    "    for match in matches:\n",
    "        if not any(re.search(pattern, match) for pattern in hint_patterns):\n",
    "            explanations.append(match)\n",
    "\n",
    "    # 결과 저장\n",
    "    with open(output_path, 'w', encoding='utf-8') as f:\n",
    "        for explanation in explanations:\n",
    "            f.write(explanation + '\\n\\n')\n",
    "    \n",
    "    return len(explanations)\n",
    "\n",
    "pdf_path = glob.glob(\"test_data/*.pdf\")\n",
    "output_path = \"지구과학.txt\"  # 저장할 파일 경로\n",
    "\n",
    "count = extract_explanations(pdf_path[0], output_path)\n",
    "print(f\"총 {count}개의 해설을 추출하여 {output_path}에 저장했습니다.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "총 114개의 해설을 추출하여 정치와법.txt에 저장했습니다.\n"
     ]
    }
   ],
   "source": [
    "### 정치와 법\n",
    "from langchain_community.document_loaders import PyMuPDFLoader\n",
    "import re\n",
    "\n",
    "def extract_explanations(pdf_path, output_path):\n",
    "    \"\"\"\n",
    "    PDF에서 \"해설 | \"로 시작하고 마침표로 끝나는 문장을 추출하여 파일로 저장하는 함수\n",
    "    \n",
    "    Args:\n",
    "        pdf_path (str): 입력 PDF 파일 경로\n",
    "        output_path (str): 출력 텍스트 파일 경로\n",
    "    \"\"\"\n",
    "    # PDF 로드\n",
    "    loader = PyMuPDFLoader(pdf_path)\n",
    "    pages = loader.load()\n",
    "    start_word = \"정답 찾기\"\n",
    "    end_word = \"오답 피하기\"\n",
    "    pattern = rf\"{start_word}.*?{end_word}\"\n",
    "    matches = []\n",
    "    for page in pages:\n",
    "        matches.extend(re.findall(pattern, page.page_content, re.DOTALL))\n",
    "\n",
    "    matches = [s[len(start_word):-len(end_word)] for s in matches]\n",
    "    \n",
    "    hint_patterns = [r\"\\d\", r\"㉠\",  r\"[a-zA-Z]\", r\"\\(가\\)\", r\"\\(나\\)\",r\"ㄱ.\",r\"ㄴ.\",r\"ㄷ.\",r\"ㄹ.\",]\n",
    "    explanations = []\n",
    "    for match in matches:\n",
    "        # hint_patterns에 해당하는 패턴이 없는 경우에만 추가\n",
    "        if not any(re.search(pattern, match) for pattern in hint_patterns):\n",
    "            explanations.append(match)\n",
    "    \n",
    "    # r\"①\", r\"②\", r\"③\", r\"④\", r\"⑤\"는 제거\n",
    "    explanations = [re.sub(r\"①|②|③|④|⑤\", \"\", explanation) for explanation in explanations]\n",
    "    \n",
    "    # 결과 저장\n",
    "    with open(output_path, 'w', encoding='utf-8') as f:\n",
    "        for explanation in explanations:\n",
    "            f.write(explanation)\n",
    "    \n",
    "    return len(explanations)\n",
    "\n",
    "pdf_path = glob.glob(\"test_data/*.pdf\")\n",
    "output_path = \"정치와법.txt\"  # 저장할 파일 경로\n",
    "\n",
    "count = extract_explanations(pdf_path[0], output_path)\n",
    "print(f\"총 {count}개의 해설을 추출하여 {output_path}에 저장했습니다.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_community.document_loaders import PyMuPDFLoader\n",
    "from langchain_experimental.text_splitter import SemanticChunker\n",
    "from langchain_upstage import UpstageEmbeddings\n",
    "def split_pdf_to_sentences(pdf_path, output_file):\n",
    "    # PDF 파일 로드\n",
    "    loader = PyMuPDFLoader(pdf_path)\n",
    "    documents = loader.load()\n",
    "    \n",
    "    # OpenAI 임베딩 모델 초기화 (OpenAI API 키 필요)\n",
    "    embeddings = UpstageEmbeddings(model=\"embedding-passage\")\n",
    "    \n",
    "    # Semantic Chunker 초기화 (문장 단위 분할)\n",
    "    text_splitter = SemanticChunker(\n",
    "        embeddings, \n",
    "        breakpoint_threshold_type=\"percentile\",  # 다양한 옵션 선택 가능\n",
    "        breakpoint_threshold_amount=95  # 분할 임계값 조정 가능\n",
    "    )\n",
    "    \n",
    "    # 문서 분할\n",
    "    splits = text_splitter.split_documents(documents)\n",
    "    \n",
    "    \n",
    "    # 분할된 문장을 텍스트 파일로 저장\n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        for i, split in enumerate(splits, 1):\n",
    "            f.write(split.page_content + \"\\n\\n\")\n",
    "# 사용 예시\n",
    "if __name__ == \"__main__\":\n",
    "    pdf_path = \"./test_data/2025_기출문제집_정보처리기사_필기_핵심요약.pdf\"  # PDF 파일 경로\n",
    "    output_directory = \"output_정처기.txt\"  # 출력 디렉토리\n",
    "    \n",
    "    split_pdf_to_sentences(pdf_path, output_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ground Truth 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "196"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = []\n",
    "with open(output_directory, 'r', encoding='utf-8') as f:\n",
    "    data = f.read().split(\"\\n\\n\")\n",
    "data = [d for d in data if len(d) > 300]\n",
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from pydantic import BaseModel, Field\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from langchain_core.documents import Document\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "class RAGEvalOutput(BaseModel):\n",
    "    \"\"\"틀린 문장과 피드백을 포함하는 출력 형식\"\"\"\n",
    "    modified_statement: str = Field(description=\"원본 텍스트의 핵심 개념을 포함한 문장 또는 틀린 문장\")\n",
    "    feedback: str = Field(description=\"문장의 오류가 있는지 여부 (O, X)\")\n",
    "\n",
    "def create_rag_evaluation_dataset(docsearch, num_samples=None):\n",
    "    \"\"\"\n",
    "    Pinecone 인덱스에서 문서를 가져와 RAG 평가용 데이터셋을 생성합니다.\n",
    "    LangChain을 사용하여 틀린 문장과 피드백을 한 번의 API 호출로 생성합니다.\n",
    "    \n",
    "    Args:\n",
    "        docsearch: Pinecone 벡터 스토어 객체\n",
    "        num_samples: 생성할 샘플 수 (None일 경우 전체 문서 사용)\n",
    "    \n",
    "    Returns:\n",
    "        생성된 데이터프레임\n",
    "    \"\"\"\n",
    "    # LangChain 컴포넌트 설정\n",
    "    parser = JsonOutputParser(pydantic_object=RAGEvalOutput)\n",
    "    \n",
    "    prompt = ChatPromptTemplate.from_template(\"\"\"다음 텍스트의 핵심 개념을 파악하고, \n",
    "    1) 개념을 활용한 문장을 2개 적어주세요. 이 때, 하나는 맞는 문장, 하나는 틀린 문장으로 작성해주세요.\n",
    "    2) 개념을 활용한 문장 2개에 대한 정답을 O, X로 표시해주세요\n",
    "    \n",
    "    모든 문장은 다음 텍스트의 핵심 개념을 포함해야 합니다.\n",
    "    모든 문장은 원본 텍스트와 다른 형태여야 합니다.\n",
    "    \n",
    "    원본 텍스트:\n",
    "    {text}\n",
    "    \n",
    "    아래 형식으로 응답해주세요:\n",
    "    {format_instructions}\n",
    "    \"\"\")\n",
    "    \n",
    "    # ChatOpenAI 모델 초기화\n",
    "    model = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.2)\n",
    "    \n",
    "    # 체인 구성\n",
    "    chain = prompt | model | parser\n",
    "    \n",
    "    evaluation_data = []\n",
    "    docsearch = docsearch[:num_samples] if num_samples else docsearch\n",
    "    for doc in tqdm(docsearch, desc=\"데이터셋 생성 중\"):\n",
    "        chunk_text = doc\n",
    "            \n",
    "        # LangChain을 사용하여 틀린 문장과 피드백 한 번에 생성\n",
    "        result = chain.invoke({\n",
    "            \"text\": chunk_text,\n",
    "            \"format_instructions\": parser.get_format_instructions()\n",
    "        })\n",
    "        try:\n",
    "            for res in result:\n",
    "                evaluation_data.append({\n",
    "                    'context': chunk_text,\n",
    "                    'question': res[\"modified_statement\"],\n",
    "                    'answer': res[\"feedback\"],\n",
    "                })\n",
    "        except:\n",
    "            pass\n",
    "    # DataFrame 생성\n",
    "    df = pd.DataFrame(evaluation_data)\n",
    "    \n",
    "    # CSV 파일로 저장\n",
    "    df.to_csv('정처기OX.csv', index=False)\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "evaluation_df = create_rag_evaluation_dataset(data)\n",
    "print(f\"생성된 데이터셋 크기: {len(evaluation_df)}\")\n",
    "print(\"\\n데이터셋 샘플:\")\n",
    "print(evaluation_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solar LLM 테스트 (without RAG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_upstage import ChatUpstage\n",
    "import pandas as pd\n",
    "import os\n",
    "from typing import List\n",
    "import time\n",
    "import re\n",
    "\n",
    "def create_batch_prompt(questions: List[str], batch_size: int = 5) -> str:\n",
    "    \"\"\"여러 질문을 하나의 프롬프트로 만듭니다.\"\"\"\n",
    "    numbered_questions = [f\"Q{i+1}. {q}\" for i, q in enumerate(questions)]\n",
    "    questions_text = \"\\n\".join(numbered_questions)\n",
    "    \n",
    "    return f\"\"\"\n",
    "    다음 문장들의 사실 여부를 판단해주세요.\n",
    "    응답 형식: Q1: O 또는 X, Q2: O 또는 X, ..., Qn: O 또는 X\n",
    "    \n",
    "    {questions_text}\n",
    "    \"\"\"\n",
    "\n",
    "def parse_batch_response(response: str, batch_size: int) -> List[str]:\n",
    "    \"\"\"LLM의 응답을 개별 답변으로 파싱합니다.\"\"\"\n",
    "    try:\n",
    "        pattern = r\"Q(\\d+):\\s*(O|X)\"\n",
    "        matches = re.findall(pattern, response)\n",
    "        answer = [ans for _, ans in matches if ans in ['O', 'X']]\n",
    "        if len(answer) < batch_size:\n",
    "            print(f\"응답 개수({len(answer)})가 배치 크기({batch_size})보다 작습니다.\")\n",
    "            raise ValueError(\"응답 개수 부족\")\n",
    "        return answer[:batch_size]\n",
    "    except Exception as e:\n",
    "        print(f\"응답 파싱 중 오류 발생: {e}\")\n",
    "        return ['-'] * batch_size  # 오류 시 기본값 반환\n",
    "\n",
    "def process_questions_in_batches(df: pd.DataFrame, batch_size: int = 5, retry_delay: int = 1) -> pd.DataFrame:\n",
    "    \"\"\"데이터프레임의 질문들을 배치로 처리합니다.\"\"\"\n",
    "    llm_upstage = ChatUpstage(\n",
    "        api_key=os.environ.get(\"UPSTAGE_API_KEY\"),\n",
    "        temperature=0,\n",
    "        model=\"solar-pro\"\n",
    "    )\n",
    "    \n",
    "    all_answers = []\n",
    "    questions = df['question'].tolist()\n",
    "    \n",
    "    for i in range(0, len(questions), batch_size):\n",
    "        batch_questions = questions[i:i + batch_size]\n",
    "        prompt = create_batch_prompt(batch_questions, batch_size)\n",
    "        \n",
    "        max_retries = 3\n",
    "        for attempt in range(max_retries):\n",
    "            try:\n",
    "                response = llm_upstage(prompt).content\n",
    "                batch_answers = parse_batch_response(response, len(batch_questions))\n",
    "                all_answers.extend(batch_answers)\n",
    "                print(f\"배치 {i//batch_size + 1} 처리 완료: {len(batch_questions)}개 질문\")\n",
    "                break\n",
    "            except Exception as e:\n",
    "                print(f\"배치 처리 중 오류 발생 (시도 {attempt + 1}/{max_retries}): {e}\")\n",
    "                if attempt < max_retries - 1:\n",
    "                    time.sleep(retry_delay)\n",
    "                else:\n",
    "                    all_answers.extend(['-'] * len(batch_questions))  # 오류 시 기본값\n",
    "        \n",
    "        time.sleep(retry_delay)  # API 요청 간 딜레이\n",
    "    \n",
    "    df_result = df.copy()\n",
    "    df_result['generated_answer'] = all_answers\n",
    "    return df_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CSV 파일 읽기\n",
    "science = pd.read_csv(\"지구과학OX.csv\")\n",
    "# 순서 섞기\n",
    "science = science.sample(frac=1, random_state=42)\n",
    "science = science.head(200)\n",
    "# 배치 처리 실행\n",
    "batch_size = 10  # 한 번에 처리할 질문 수\n",
    "processed_df = process_questions_in_batches(science, batch_size)\n",
    "\n",
    "# 결과 저장\n",
    "processed_df.to_csv(\"정치와법OX_upstage_batch.csv\", index=False)\n",
    "\n",
    "# Error 비율\n",
    "error_rate = (processed_df['generated_answer'] == '-').mean() * 100\n",
    "print(f\"Error 비율: {error_rate:.5f}%\")\n",
    "# Error row 제거\n",
    "processed_df = processed_df[processed_df['generated_answer'] != '-']\n",
    "\n",
    "# 정확도 계산\n",
    "accuracy = (processed_df['answer'] == processed_df['generated_answer']).mean() * 100\n",
    "print(f\"정답 비율: {accuracy:.5f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solar LLM with RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_upstage import ChatUpstage, UpstageEmbeddings\n",
    "from langchain_pinecone import PineconeVectorStore\n",
    "import pandas as pd\n",
    "import os\n",
    "from typing import List\n",
    "import time\n",
    "import re\n",
    "\n",
    "class RAGScienceQA:\n",
    "    def __init__(\n",
    "        self,\n",
    "        index_name: str,\n",
    "        embedding_model,\n",
    "        batch_size: int = 5,\n",
    "        k_similar: int = 3\n",
    "    ):\n",
    "        self.batch_size = batch_size\n",
    "        self.k_similar = k_similar\n",
    "        \n",
    "        # Initialize LLM\n",
    "        self.llm = ChatUpstage(\n",
    "            api_key=os.environ.get(\"UPSTAGE_API_KEY\"),\n",
    "            temperature=0,\n",
    "            model=\"solar-pro\"\n",
    "        )\n",
    "        \n",
    "        # Initialize vector store\n",
    "        self.vector_store = PineconeVectorStore.from_existing_index(\n",
    "            index_name=index_name,\n",
    "            embedding=embedding_model\n",
    "        )\n",
    "\n",
    "    def get_relevant_context(self, question: str) -> str:\n",
    "        \"\"\"주어진 질문과 관련된 컨텍스트를 검색합니다.\"\"\"\n",
    "        similar_docs = self.vector_store.similarity_search(\n",
    "            question,\n",
    "            k=self.k_similar\n",
    "        )\n",
    "        \n",
    "        # 검색된 문서들을 하나의 컨텍스트로 결합\n",
    "        context = \"\\n\".join([doc.page_content for doc in similar_docs])\n",
    "        return context\n",
    "\n",
    "    def create_rag_prompt(self, questions: List[str]) -> str:\n",
    "        \"\"\"RAG 기반 프롬프트를 생성합니다.\"\"\"\n",
    "        # 각 질문에 대한 컨텍스트 검색\n",
    "        question_contexts = []\n",
    "        for i, q in enumerate(questions, start=1):\n",
    "            context = self.get_relevant_context(q)\n",
    "            question_contexts.append(f\"Q{i}: {q}\\nC{i}: {context}\")\n",
    "            \n",
    "        # 번호가 매겨진 질문과 컨텍스트 생성\n",
    "        qa_text = \"\\n\\n\".join(question_contexts)\n",
    "        \n",
    "        return f\"\"\"\n",
    "        주어진 각 질문(Q)에 대해 관련 정보(C)를 참고하여 사실 여부를 판단해주세요.\n",
    "        응답 형식: Q1: O 또는 X, Q2: O 또는 X, ..., Qn: O 또는 X\n",
    "        \n",
    "        {qa_text}\n",
    "        \"\"\"\n",
    "\n",
    "    def parse_batch_response(self, response: str, batch_size: int) -> List[str]:\n",
    "        \"\"\"LLM의 응답을 개별 답변으로 파싱합니다.\"\"\"\n",
    "        try:\n",
    "            pattern = r\"Q(\\d+):\\s*(O|X)\"\n",
    "            matches = re.findall(pattern, response)\n",
    "            answer = [ans for _, ans in matches if ans in ['O', 'X']]\n",
    "            if len(answer) < batch_size:\n",
    "                print(f\"응답 개수({len(answer)})가 배치 크기({batch_size})보다 작습니다.\")\n",
    "                raise ValueError(\"응답 개수 부족\")\n",
    "            return answer[:batch_size]\n",
    "        except Exception as e:\n",
    "            print(f\"응답 파싱 중 오류 발생: {e}\")\n",
    "            return ['-'] * batch_size\n",
    "\n",
    "    def process_questions(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"데이터프레임의 질문들을 RAG를 사용하여 처리합니다.\"\"\"\n",
    "        all_answers = []\n",
    "        questions = df['question'].tolist()\n",
    "        \n",
    "        for i in range(0, len(questions), self.batch_size):\n",
    "            batch_questions = questions[i:i + self.batch_size]\n",
    "            prompt = self.create_rag_prompt(batch_questions)\n",
    "            \n",
    "            max_retries = 3\n",
    "            for attempt in range(max_retries):\n",
    "                try:\n",
    "                    response = self.llm(prompt).content\n",
    "                    batch_answers = self.parse_batch_response(\n",
    "                        response,\n",
    "                        len(batch_questions)\n",
    "                    )\n",
    "                    all_answers.extend(batch_answers)\n",
    "                    print(f\"배치 {i//self.batch_size + 1} 처리 완료: {len(batch_questions)}개 질문\")\n",
    "                    break\n",
    "                except Exception as e:\n",
    "                    print(f\"배치 처리 중 오류 발생 (시도 {attempt + 1}/{max_retries}): {e}\")\n",
    "                    if attempt < max_retries - 1:\n",
    "                        time.sleep(1)\n",
    "                    else:\n",
    "                        all_answers.extend(['-'] * len(batch_questions)) # Error 시 기본값\n",
    "            \n",
    "            time.sleep(1)  # API 요청 간 딜레이\n",
    "        \n",
    "        df_result = df.copy()\n",
    "        df_result['generated_answer'] = all_answers\n",
    "        return df_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # 환경 설정\n",
    "index_name = \"dev-01\"\n",
    "\n",
    "# OpenAI 임베딩 모델 초기화 (또는 다른 임베딩 모델 사용)\n",
    "embeddings_query = UpstageEmbeddings(model=\"embedding-query\") #4096\n",
    "\n",
    "\n",
    "# RAG 시스템 초기화\n",
    "rag_qa = RAGScienceQA(\n",
    "    index_name=index_name,\n",
    "    embedding_model=embeddings_query,\n",
    "    batch_size=10,\n",
    "    k_similar=2\n",
    ")\n",
    "\n",
    "# 데이터 로드\n",
    "science = pd.read_csv(\"지구과학OX.csv\")\n",
    "# 순서 섞기\n",
    "science = science.sample(frac=1, random_state=42)\n",
    "science = science.head(200)\n",
    "\n",
    "# RAG 기반 처리 실행\n",
    "processed_df = rag_qa.process_questions(science)\n",
    "\n",
    "# 결과 저장\n",
    "processed_df.to_csv(\"정치와법OX_rag_upstage_dev1.csv\", index=False)\n",
    "\n",
    "# Error 비율\n",
    "error_rate = (processed_df['generated_answer'] == '-').mean() * 100\n",
    "print(f\"Error 비율: {error_rate:.5f}%\")\n",
    "# Error row 제거\n",
    "processed_df = processed_df[processed_df['generated_answer'] != '-']\n",
    "\n",
    "# 정확도 계산\n",
    "accuracy = (processed_df['answer'] == processed_df['generated_answer']).mean() * 100\n",
    "print(f\"정답 비율: {accuracy:.5f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
