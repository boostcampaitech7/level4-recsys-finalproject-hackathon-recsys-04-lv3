{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import library\n",
    "from pinecone import Pinecone, ServerlessSpec\n",
    "from langchain_upstage import  UpstageEmbeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import PyMuPDFLoader\n",
    "from langchain_core.documents import Document\n",
    "from dotenv import load_dotenv\n",
    "from kiwipiepy import Kiwi\n",
    "from collections import Counter\n",
    "from typing import List, Dict, Tuple\n",
    "import glob\n",
    "import os\n",
    "import hashlib\n",
    "import math\n",
    "import re\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "api_key = os.environ.get(\"PINECONE_API_KEY\")\n",
    "\n",
    "pc = Pinecone(api_key=api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hybrid is already exists.\n"
     ]
    }
   ],
   "source": [
    "index_name = \"hybrid\"\n",
    "if index_name not in [index_info[\"name\"] for index_info in pc.list_indexes()]:\n",
    "    pc.create_index(\n",
    "        name=index_name,\n",
    "        dimension=4096, \n",
    "        metric=\"dotproduct\",\n",
    "        spec=ServerlessSpec(\n",
    "            cloud=\"aws\",\n",
    "            region=\"us-east-1\"\n",
    "        ) \n",
    "    )\n",
    "    print(f\"{index_name} has been successfully created\")\n",
    "else:\n",
    "    print(f\"{index_name} is already exists.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class KoreanBM25Encoder:\n",
    "    def __init__(self, k1: float = 1.5, b: float = 0.75):\n",
    "        self.kiwi = Kiwi()\n",
    "        self.k1 = k1\n",
    "        self.b = b\n",
    "        self.avg_doc_length = 0\n",
    "        self.doc_freqs = Counter()\n",
    "        self.num_docs = 0\n",
    "        self.vocabulary = set()\n",
    "        \n",
    "    def _tokenize(self, text: str) -> List[str]:\n",
    "        \"\"\"키위를 사용하여 텍스트를 형태소 분석\"\"\"\n",
    "        tokens = []\n",
    "        result = self.kiwi.analyze(text)\n",
    "        for token, pos, _, _ in result[0][0]:\n",
    "            if pos.startswith('N') or pos.startswith('V') or pos.startswith('MA'):\n",
    "                tokens.append(f\"{token}_{pos}\")\n",
    "        return tokens\n",
    "    \n",
    "    def fit(self, documents: List[str]):\n",
    "        \"\"\"문서 집합으로부터 BM25 통계 계산\"\"\"\n",
    "        doc_lengths = []\n",
    "        term_freqs = []\n",
    "        \n",
    "        for doc in documents:\n",
    "            tokens = self._tokenize(doc)\n",
    "            doc_lengths.append(len(tokens))\n",
    "            term_freq = Counter(tokens)\n",
    "            term_freqs.append(term_freq)\n",
    "            \n",
    "            self.doc_freqs.update(term_freq.keys())\n",
    "            self.vocabulary.update(tokens)\n",
    "        \n",
    "        self.num_docs = len(documents)\n",
    "        self.avg_doc_length = sum(doc_lengths) / self.num_docs if self.num_docs > 0 else 0\n",
    "        \n",
    "    def encode_sparse(self, text: str) -> Tuple[List[int], List[float]]:\n",
    "        \"\"\"텍스트를 sparse vector로 인코딩 - 리스트 형태로 반환\"\"\"\n",
    "        tokens = self._tokenize(text)\n",
    "        term_freq = Counter(tokens)\n",
    "        \n",
    "        indices = []\n",
    "        values = []\n",
    "        \n",
    "        doc_length = len(tokens)\n",
    "        \n",
    "        for term, freq in term_freq.items():\n",
    "            if term in self.vocabulary:\n",
    "                idf = math.log((self.num_docs - self.doc_freqs[term] + 0.5) /\n",
    "                             (self.doc_freqs[term] + 0.5) + 1.0)\n",
    "                \n",
    "                numerator = freq * (self.k1 + 1)\n",
    "                denominator = freq + self.k1 * (1 - self.b + self.b * doc_length / self.avg_doc_length)\n",
    "                \n",
    "                bm25_score = idf * (numerator / denominator)\n",
    "                \n",
    "                index = abs(hash(term)) % (10 ** 9)\n",
    "                indices.append(index)\n",
    "                values.append(float(bm25_score))\n",
    "        \n",
    "        return indices, values\n",
    "\n",
    "def remove_question_sentences(paragraph):\n",
    "    \"\"\"불필요한 문장 제거\"\"\"\n",
    "    result = re.sub(r'[^.?!]*[\\?]|[^.?!]*보자\\.', '', paragraph)\n",
    "    result = re.sub(r'\\s+', ' ', result).strip()\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class HybridSearch:\n",
    "    def __init__(self, index_name: str, api_key: str):\n",
    "        \"\"\"Pinecone hybrid search 초기화\"\"\"\n",
    "        pc = Pinecone(api_key=api_key)\n",
    "        self.index = pc.Index(index_name)\n",
    "        self.bm25_encoder = KoreanBM25Encoder()\n",
    "        self.embeddings = UpstageEmbeddings(model=\"embedding-passage\")\n",
    "        self.embeddings_query =  UpstageEmbeddings(model=\"embedding-query\")\n",
    "        \n",
    "    def index_documents(self, documents: List[Document]):\n",
    "        \"\"\"Document 객체 처리 및 인덱싱\"\"\"\n",
    "        # 문서 전처리\n",
    "        processed_documents = []\n",
    "        processed_texts = []\n",
    "        \n",
    "        for doc in documents:\n",
    "            processed_text = remove_question_sentences(doc.page_content)\n",
    "            if processed_text.strip():\n",
    "                processed_documents.append(doc)\n",
    "                processed_texts.append(processed_text)\n",
    "                \n",
    "        if not processed_documents:\n",
    "            return\n",
    "        \n",
    "        # Dense vectors 생성\n",
    "        dense_vectors = self.embeddings.embed_documents(processed_texts)\n",
    "        \n",
    "        # BM25 encoder 학습\n",
    "        self.bm25_encoder.fit(processed_texts)\n",
    "        \n",
    "        # 각 문서를 인덱싱\n",
    "        vectors_to_upsert = []\n",
    "        for doc, text, dense_vec in zip(processed_documents, processed_texts, dense_vectors):\n",
    "            indices, values = self.bm25_encoder.encode_sparse(text)\n",
    "            if not indices: continue # upsert 하지 않음.\n",
    "            # 메타데이터 준비\n",
    "            metadata = {\n",
    "                'page': doc.metadata.get('page', ''),\n",
    "                'source': doc.metadata.get('source', ''),\n",
    "                # 'subject': doc.metadata.get('subject', ''),\n",
    "                'text': text\n",
    "            }\n",
    "            \n",
    "            vectors_to_upsert.append({\n",
    "                'id': hashlib.md5(text.encode()).hexdigest(),\n",
    "                'values': dense_vec,\n",
    "                'sparse_values': {\n",
    "                    'indices': indices,\n",
    "                    'values': values\n",
    "                },\n",
    "                'metadata': metadata\n",
    "            })\n",
    "            \n",
    "            # 배치 처리\n",
    "            if len(vectors_to_upsert) >= 100:\n",
    "                self.index.upsert(vectors=vectors_to_upsert)\n",
    "                vectors_to_upsert = []\n",
    "        \n",
    "        # 남은 문서들 처리\n",
    "        if vectors_to_upsert:\n",
    "            self.index.upsert(vectors=vectors_to_upsert)\n",
    "    \n",
    "    def search(self, query: str, alpha: float = 0.5, top_k: int = 5) -> List[Dict]:\n",
    "        \"\"\"하이브리드 검색 수행\"\"\"\n",
    "        dense_vector = self.embeddings_query.embed_query(query)\n",
    "        indices, values = self.bm25_encoder.encode_sparse(query)\n",
    "        \n",
    "        if alpha < 0 or alpha > 1:\n",
    "            raise ValueError(\"Alpha must be between 0 and 1\")\n",
    "        hs = {\n",
    "            'indices': indices,\n",
    "            'values':  [v * (1 - alpha) for v in values]\n",
    "        }\n",
    "        print(hs)\n",
    "        dense_vector = [v * alpha for v in dense_vector]\n",
    "        \n",
    "        results = self.index.query(\n",
    "            vector=dense_vector,\n",
    "            sparse_vector= hs,\n",
    "            top_k=top_k,\n",
    "            include_metadata=True\n",
    "        )\n",
    "        \n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/venv/lib/python3.12/site-packages/langchain_community/document_loaders/parsers/pdf.py:322: UserWarning: Warning: Empty content on page 225 of document data/천재교육_고등교과서_지구과학Ⅱ_오필석(15개정)_교과서 본문.pdf\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "719"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "INDEX_NAME = \"hybrid\"\n",
    "API_KEY = os.environ.get(\"PINECONE_API_KEY\")\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=150)\n",
    "\n",
    "split_docs = []\n",
    "files = sorted(glob.glob(\"data/*.pdf\"))\n",
    "\n",
    "for file in files:\n",
    "    loader = PyMuPDFLoader(file)\n",
    "    split_docs.extend(loader.load_and_split(text_splitter))\n",
    "\n",
    "# 문서 개수 확인\n",
    "len(split_docs)\n",
    "# 샘플 데이터\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "searcher = HybridSearch(INDEX_NAME, API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "searcher.index_documents(split_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'indices': [461132265, 863232857, 36473833, 400955895, 462028729, 824163176, 6541746], 'values': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}\n"
     ]
    }
   ],
   "source": [
    "query = \"피해 사례 쓰나미 과학 나미와 폭풍 해일에 의 한 피해를 같이\"\n",
    "results = searcher.search(query, alpha=1, top_k=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'matches': [{'id': 'd8f85d85a8abfb13b34697f1032df000',\n",
       "              'metadata': {'page': 107.0,\n",
       "                           'source': 'data/천재교육_고등교과서_지구과학Ⅱ_오필석(15개정)_교과서 '\n",
       "                                     '본문.pdf',\n",
       "                           'text': '•해일이 발생하는 여러 가지 원인을 이해할 수 있다. •해일에 의한 피해 '\n",
       "                                   '사례와 대처 방안을 조사하여 발표할 수 있다. 02 해파는 보통 해수면 위를 '\n",
       "                                   '부는 바람에 의해서 만들어지지만, 태풍이나 지진에 의해 형성되는 파장이 길고 '\n",
       "                                   '파고가 높은 해파도 있다. 이러한 해파를 해일이라고 한다. 해 일은 연안으로 '\n",
       "                                   '접근하면서 높은 파도를 만들어 큰 피해를 주기도 한다. 해일 '\n",
       "                                   '쓰나미(tsunami) 일본에서는 항구의 파도라는 뜻 으로 큰 해일을 '\n",
       "                                   '지칭하는 용어이 다. 1946년 알류샨 열도 지진과 함께 발생한 해일로 '\n",
       "                                   '하와이에 큰 피해가 발생했는데, 일본계 미국 인이 쓰나미라는 용어를 사용하 '\n",
       "                                   '면서 국제 사회에서 공식 명칭으 로 쓰이기 시작하였다. 스스로 생각해 보기 '\n",
       "                                   '로 해 그림은 쓰나미가 나타난 해안의 모습이다. 쓰나미가 나타나기 직전에 '\n",
       "                                   '해안의 물이 빠 른 속도로 빠져나갔다가 거대한 파도가 되어 해안가를 '\n",
       "                                   '뒤덮었다.'},\n",
       "              'score': 0.450205684,\n",
       "              'values': []},\n",
       "             {'id': '5b7804e05665ddda08b61be73692de88',\n",
       "              'metadata': {'page': 109.0,\n",
       "                           'source': 'data/천재교육_고등교과서_지구과학Ⅱ_오필석(15개정)_교과서 '\n",
       "                                     '본문.pdf',\n",
       "                           'text': '쓰나미와 폭풍 해일에 의 한 피해를 같이 조사하면 좋겠어. 목표 쓰나미에 '\n",
       "                                   '의한 피해 사례를 조사 하여 전파 과정의 특징을 분석하 고, 대처 방안을 '\n",
       "                                   '제시할 수 있다. 과정 1 (가), (나)를 활용하여 인도양에서 발생한 '\n",
       "                                   '쓰나미가 수마트라의 반다아체, 푸껫, 스리랑 카에 도착하는 데 걸린 시간과 '\n",
       "                                   '파고를 분석한다. 2 이 쓰나미로 피해를 입은 나라와 피해가 컸던 까닭, 그 '\n",
       "                                   '지역에서 실시한 대처 방안을 조 사한다. 진앙 반다아체 푸껫 스리랑카 '\n",
       "                                   '반다아체 푸껫 스리랑카 진앙 0'},\n",
       "              'score': 0.447931349,\n",
       "              'values': []},\n",
       "             {'id': '077d33fec3ea4cdf91b979dbac1ed352',\n",
       "              'metadata': {'page': 109.0,\n",
       "                           'source': 'data/천재교육_고등교과서_지구과학Ⅱ_오필석(15개정)_교과서 '\n",
       "                                     '본문.pdf',\n",
       "                           'text': '사한다. 진앙 반다아체 푸껫 스리랑카 반다아체 푸껫 스리랑카 진앙 0 5 '\n",
       "                                   '10 12 0 5 10 100 1738 시간(시) 파고(cm) (가) 쓰나미 '\n",
       "                                   '도착 시간 (나) 파고 쓰나미(지진 해일)의 피해 사례와 대처 방안 조사 및 '\n",
       "                                   '발표하기 자료 해석 의사소통 능력 문제 해결력 문 탐구 능력 그림 (가), '\n",
       "                                   '(나)는 2004년 인도양에서 발생한 쓰나미의 전파 경로와 파고를 나타낸 '\n",
       "                                   '것이다. 쓰나미 찾아보기 〈출처: 미국해양대기청(NOAA), 2016〉 '\n",
       "                                   '110 IV . 해수의 운동과 순환'},\n",
       "              'score': 0.42009148,\n",
       "              'values': []}],\n",
       " 'namespace': '',\n",
       " 'usage': {'read_units': 11}}"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
