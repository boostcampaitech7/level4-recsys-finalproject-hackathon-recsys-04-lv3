{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import library\n",
    "from pinecone import Pinecone\n",
    "from langchain_upstage import ChatUpstage, UpstageEmbeddings\n",
    "from dotenv import load_dotenv\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "\n",
    "index_name = \"quickstart\"\n",
    "client = Pinecone(api_key=os.environ.get(\"PINECONE_API_KEY\"), source_tag=\"langchain\")\n",
    "llm_upstage = ChatUpstage(api_key=os.environ.get(\"UPSTAGE_API_KEY\"), temperature=0)\n",
    "embeddings_query = UpstageEmbeddings(model=\"embedding-query\") #4096"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever_prompt_template = \"\"\"\n",
    "\\n\\nHuman: Here is the context information, inside <context></context> XML tags.\n",
    "\n",
    "<context>\n",
    "{context}\n",
    "</context>\n",
    "\n",
    "Given the context information and not prior knowledge.\n",
    "generate only questions based on the below query.\n",
    "\n",
    "You are a Professor. Your task is to setup \\\n",
    "{num_questions_per_chunk} questions for an upcoming \\\n",
    "quiz/examination. The questions should be diverse in nature \\\n",
    "across the document. The questions should not contain options, start with \"-\"\n",
    "Restrict the questions to the context information provided.\n",
    "\n",
    "\\n\\nAssistant:\"\"\"\n",
    "\n",
    "PROMPT_RETRIEVER = PromptTemplate(\n",
    "    template=retriever_prompt_template, input_variables=[\"context\", \"num_questions_per_chunk\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "generation_prompt_template = \"\"\"\n",
    "Here is the context, inside <context></context> XML tags.\n",
    "\n",
    "<context>\n",
    "{context}\n",
    "</context>\n",
    "\n",
    "Only using the context as above, answer the following question with the rules as below:\n",
    "    - Don't insert XML tag such as <context> and </context> when answering.\n",
    "    - Write as much as you can\n",
    "    - Be courteous and polite\n",
    "    - Only answer the question if you can find the answer in the context with certainty.\n",
    "    - Skip the preamble\n",
    "    - Use three sentences maximum and keep the answer concise.\n",
    "    - If the answer is not in the context, just say \"Could not find answer in given contexts.\"\n",
    "\n",
    "Question:\n",
    "{question}\n",
    "\n",
    "Answer:\"\"\"\n",
    "\n",
    "PROMPT_GENERATION = PromptTemplate(\n",
    "    template=generation_prompt_template, input_variables=[\"context\", \"question\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = client.Index(index_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GTGenerator(index, llm_retriever, llm_generation, prompt_retriever, prompt_generation, batch_size=50, max_batch=2, num_questions_per_chunk=1):\n",
    "\n",
    "    llm_chain_retriever = LLMChain(llm=llm_retriever, prompt=prompt_retriever)\n",
    "    llm_chain_generation = LLMChain(llm=llm_generation, prompt=prompt_generation)\n",
    "    gt = [] # [question, 정답 id, 정답 text]\n",
    "\n",
    "    all_ids = list(index.list(limit=batch_size))\n",
    "    \n",
    "    if max_batch < len(all_ids):\n",
    "        all_ids = all_ids[:max_batch]    \n",
    "\n",
    "    # ID를 기반으로 데이터를 하나씩 가져오기\n",
    "    for fetched_ids in all_ids:\n",
    "        fetched_docs = index.fetch(ids=fetched_ids)\n",
    "        fetched_docs = fetched_docs.vectors\n",
    "        \n",
    "        for doc_id in fetched_ids:\n",
    "            doc_text =  fetched_docs[doc_id][\"metadata\"][\"text\"]\n",
    "\n",
    "            questions = llm_chain_retriever.predict(context=doc_text, num_questions_per_chunk=str(num_questions_per_chunk))\n",
    "            \n",
    "            questions = questions.split(\"\\n\\n-\")\n",
    "            if len(questions) <= num_questions_per_chunk + 1:\n",
    "\n",
    "                if len(questions) == num_questions_per_chunk:\n",
    "                    questions = list(map(lambda x:x.strip(), questions))\n",
    "                else:\n",
    "                    questions = list(map(lambda x:x.strip(), questions[1:]))\n",
    "                for q in questions:\n",
    "                    answer = llm_chain_generation.predict(question=q, context=doc_text)\n",
    "                    answer = answer.strip()\n",
    "                    gt.append([q, answer, doc_id, doc_text])\n",
    "            else:\n",
    "                print (\"err\")\n",
    "                print (questions)\n",
    "\n",
    "    return gt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "gt = GTGenerator(\n",
    "    index = index,\n",
    "    llm_retriever=llm_upstage,\n",
    "    llm_generation=llm_upstage,\n",
    "    prompt_retriever=PROMPT_RETRIEVER,\n",
    "    prompt_generation=PROMPT_GENERATION,\n",
    "    batch_size=50,\n",
    "    max_batch=3,\n",
    "    num_questions_per_chunk=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "eval_dataset_retriever = pd.DataFrame(gt, columns=[\"question\", \"answer\", \"doc_id\", \"doc\"])\n",
    "eval_dataset_retriever.to_csv(\"eval_dataset.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
