{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chunk 기반 RAG\n",
    "\n",
    "청크단위 RAG 베이스라인 만들기\n",
    "\n",
    "LLM 플로우\n",
    "\n",
    "- OCR로 text 생성 → 청크단위로 분할 → 하나의 LLM(w RAG)을 이용해서 하나의 답변 생성 → API 변환\n",
    "내용\n",
    "\n",
    "- 각 청크별로 RAG 검색하여 (n x 청크 수)개의 검색 자료를 이용해서 하나의 LLM 답변을 생성함 (n: 검색 수(default=1))\n",
    "\n",
    "https://github.com/boostcampaitech7/level4-recsys-finalproject-hackathon-recsys-04-lv3/issues/42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from glob import glob\n",
    "\n",
    "# from app.core.config import settings\n",
    "from fastapi import HTTPException\n",
    "from langchain_community.document_loaders import PyMuPDFLoader\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_pinecone import PineconeVectorStore\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_upstage import ChatUpstage, UpstageEmbeddings\n",
    "from pinecone import Pinecone, ServerlessSpec\n",
    "# from utils import logging\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# import os\n",
    "# import requests\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# from langchain_upstage import UpstageDocumentParseLoader\n",
    "# from langchain_upstage import UpstageEmbeddings\n",
    "# from langchain_upstage import ChatUpstage\n",
    "\n",
    "# from pinecone import Pinecone, ServerlessSpec\n",
    "# from langchain_pinecone import PineconeVectorStore\n",
    "# # from langchain_community.vectorstores import FAISS\n",
    "# from langchain_community.document_loaders import PyMuPDFLoader\n",
    "# from langchain_core.output_parsers import StrOutputParser\n",
    "# from langchain_core.runnables import RunnablePassthrough\n",
    "# from langchain_core.prompts import PromptTemplate\n",
    "# from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "# from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "# from langchain_teddynote.community.pinecone import create_index\n",
    "\n",
    "from rag_utils import logging\n",
    "\n",
    "# API 키 정보 로드\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ocr_txt = '''\n",
    "별의 형성 성운 단계:성운(가스와 먼지 구름)이 온도 하강으로 인해 수축이 시작.중심부가 차가워지며 밀도가 높아지고, 원시별이 형성됨. \n",
    "원시별 단계:핵융합 반응이 바로 시작되며 중심부 온도가 약 1천만 K로 상승. \n",
    "주계열성 단계 핵융합 시작:중심부 온도가 약 10억 K에 도달하면 수소가 헬륨으로 융합되며 에너지를 방출. \n",
    "특징:별은 내부의 중력과 외부로 나가는 방사압이 균형을 이루지 못하며 진동을 반복.질량이 클수록 수명이 길어지고 더 밝으며 온도는 낮아짐. \n",
    "거성 또는 초거성 단계 핵융합 종료 후 부풀어오름:중심부의 수소가 모두 소모되면 별 전체가 서서히 수축하며 적색 거성 또는 초거성이 됨. \n",
    "핵융합 반복:질량이 큰 별은 헬륨 이후에도 탄소, 산소, 규소 등을 융합하며 금까지 생성. \n",
    "별의 최후: 질량에 따른 진화(1) 질량이 작은 별 (태양과 비슷한 별) 행성상 성운:핵융합 종료 후 바깥층이 우주로 방출되지 않고 중심부로 흡수됨. \n",
    "백색왜성:중심부가 폭발하여 작은 규모의 초신성이 형성됨.(2) 질량이 큰 별 초신성 폭발:핵융합으로 헬륨까지만 생성된 후 더 이상 에너지를 방출할 수 없어 중력이 붕괴. \n",
    "중성자별:초신성 폭발 후 남은 별의 중심부가 수소로 이루어진 별로 변환됨. \n",
    "블랙홀:질량이 큰 별은 외곽층이 모두 날아가며, 작은 블랙홀이 형성됨. \n",
    "별의 진화 요약 \n",
    "질량이 작은 별 → 초거성 → 행성상 성운 → 백색왜성 질량이 큰 별 → 적색 거성 → 초신성 → (질량에 따라) 행성상 성운 또는 중성자별\n",
    "'''\n",
    "\n",
    "\n",
    "ocr_txt = \"\"\"형법\n",
    "범죄 : 국가가 형벌로 처벌할 수 있는 행위\n",
    "공권력 : 국가나 공공단체가 우월한 의사의 주체로서 국민에 대하여 명령하거나 강제하는 권력\n",
    "죄형 법정주의\n",
    "-`법률이 없으면 범죄도 없고 형벌도 없다' 라는 근대 형법의 기본원리\n",
    "국가가 범죄를 정하고 형벌을 부과하는 효과\n",
    "1 응보론\n",
    "2 일반 예방론\n",
    "3 특수 예방론\n",
    "형법의 기능\n",
    "1 법익 보호\n",
    "2 국민을 범죄로부터 보도\n",
    "3 국민의 권리 보장 \n",
    "\"\"\"\n",
    "\n",
    "ocr_txt = \"\"\"범죄의 성립요건\n",
    "- 구체적인 행위가 없어도 범죄는 성립된다\n",
    "- 실수로 법에서 금지하는 결과를 초래했을 때 법에서는 이를 고의 라고 한다\n",
    "- 범죄자가 의도한 대로 범리가 완전히 성립한 경우는 미수범 이라고 한다\n",
    "위법성 조각 사유 종류\n",
    "- 정당 방위\n",
    "- 긴급 피난\n",
    "- 자구행위\n",
    "- 피해자의 승낙\n",
    "- 정당 행위\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 텍스트를 청크 단위로 분할하기 - 시멘틱 청커(SemanticChunker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_experimental.text_splitter import SemanticChunker\n",
    "# from langchain_openai.embeddings import OpenAIEmbeddings\n",
    "from langchain_upstage import UpstageEmbeddings\n",
    "\n",
    "embeddings = UpstageEmbeddings(model=\"embedding-passage\")\n",
    "\n",
    "text_splitter = SemanticChunker(\n",
    "    embeddings=embeddings,\n",
    "    breakpoint_threshold_type=\"percentile\",\n",
    "    breakpoint_threshold_amount=95,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks = text_splitter.split_text(ocr_txt)\n",
    "\n",
    "print(type(chunks))\n",
    "print(len(chunks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, chunk in enumerate(chunks):\n",
    "    print(\"=\"*10)\n",
    "    print(f\"[Chunk {i}]\")\n",
    "    print(chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 청크별 RAG Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LANGSMITH_PROJECT_NAME = \"dev-02\"\n",
    "PINECONE_INDEX_NAME = \"dev-02\"\n",
    "\n",
    "# LangSmith 시작\n",
    "# logging.langsmith(settings.LANGSMITH_PROJECT_NAME)\n",
    "# db_index_name = settings.PINECONE_INDEX_NAME\n",
    "logging.langsmith(LANGSMITH_PROJECT_NAME)\n",
    "db_index_name = PINECONE_INDEX_NAME\n",
    "\n",
    "pc = Pinecone()\n",
    "index = pc.Index(db_index_name)\n",
    "embeddings = UpstageEmbeddings(model=\"embedding-passage\")\n",
    "vectorstore = PineconeVectorStore(index=index, embedding=embeddings)\n",
    "\n",
    "# 검색기(Retriever) 생성\n",
    "retriever = vectorstore.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 2})\n",
    "\n",
    "# 청크별 검색\n",
    "retrieved_docs = []    # Document list\n",
    "retrieved_ids = []    # RAG id list\n",
    "print(\"chunks len: \", len(chunks))\n",
    "for chunk in chunks:\n",
    "    docs = retriever.invoke(chunk)\n",
    "\n",
    "    for doc in docs:\n",
    "        doc_id = doc.id\n",
    "        if not doc_id in retrieved_ids:\n",
    "            retrieved_ids.append(doc_id)\n",
    "            retrieved_docs.append(doc)\n",
    "\n",
    "print(\"Retrieved docs len: \", len(retrieved_docs))\n",
    "print(\"Retrieved ids len: \", len(retrieved_ids))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieved_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieved_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLM 답변 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 프롬프트 생성(Create Prompt)\n",
    "prompt = PromptTemplate.from_template(\n",
    "    \"\"\"너는 입력을 보고 틀린 부분에 대해서 피드백을 주는 선생님이야\n",
    "아래 추가 조건과 정보를 참고해서 피드백 해줘\n",
    "추가 조건\n",
    "1. 참고한 정보의 이름과 페이지 알려줘\n",
    "2. 입력에 틀린 부분이 하나도 없을 경우에만 칭찬 한 문장 해줘\n",
    "3. 다른 미사여구 없이 본론부터 말해줘\n",
    "\n",
    "#정보:\n",
    "{context}\n",
    "\n",
    "#입력:\n",
    "{question}\n",
    "\n",
    "#답:\"\"\"\n",
    ")\n",
    "\n",
    "# 언어모델(LLM) 생성\n",
    "llm = ChatUpstage(model=\"solar-pro\", temperature=0.2)\n",
    "\n",
    "chain = prompt | llm | StrOutputParser()\n",
    "\n",
    "response = chain.invoke({\"context\": retrieved_docs, \"question\": ocr_txt})\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from glob import glob\n",
    "\n",
    "# from app.core.config import settings\n",
    "from fastapi import HTTPException\n",
    "from langchain_community.document_loaders import PyMuPDFLoader\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_pinecone import PineconeVectorStore\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_upstage import ChatUpstage, UpstageEmbeddings\n",
    "from langchain_experimental.text_splitter import SemanticChunker\n",
    "from pinecone import Pinecone, ServerlessSpec\n",
    "from utils import logging\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "embeddings = UpstageEmbeddings(model=\"embedding-passage\")\n",
    "\n",
    "text_splitter = SemanticChunker(\n",
    "    embeddings=embeddings,\n",
    "    breakpoint_threshold_type=\"percentile\",\n",
    "    breakpoint_threshold_amount=95,\n",
    ")\n",
    "\n",
    "chunks = text_splitter.split_text(ocr_txt)\n",
    "print(type(chunks))\n",
    "print(len(chunks))\n",
    "\n",
    "\n",
    "\n",
    "# LangSmith 시작\n",
    "logging.langsmith(settings.LANGSMITH_PROJECT_NAME)\n",
    "db_index_name = settings.PINECONE_INDEX_NAME\n",
    "\n",
    "pc = Pinecone()\n",
    "index = pc.Index(db_index_name)\n",
    "embeddings = UpstageEmbeddings(model=\"embedding-passage\")\n",
    "vectorstore = PineconeVectorStore(index=index, embedding=embeddings)\n",
    "\n",
    "# 검색기(Retriever) 생성\n",
    "retriever = vectorstore.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 2})\n",
    "\n",
    "# 청크별 검색\n",
    "retrieved_docs = []    # Document list\n",
    "retrieved_ids = []    # RAG id list\n",
    "print(\"chunks len: \", len(chunks))\n",
    "for chunk in chunks:\n",
    "    docs = retriever.invoke(chunk)\n",
    "\n",
    "    for doc in docs:\n",
    "        doc_id = doc.id\n",
    "        if not doc_id in retrieved_ids:\n",
    "            retrieved_ids.append(doc_id)\n",
    "            retrieved_docs.append(doc)\n",
    "\n",
    "print(\"Retrieved docs len: \", len(retrieved_docs))\n",
    "print(\"Retrieved ids len: \", len(retrieved_ids))\n",
    "\n",
    "\n",
    "# 프롬프트 생성(Create Prompt)\n",
    "prompt = PromptTemplate.from_template(\n",
    "    \"\"\"너는 입력을 보고 틀린 부분에 대해서 피드백을 주는 선생님이야\n",
    "아래 추가 조건과 정보를 참고해서 피드백 해줘\n",
    "추가 조건\n",
    "1. 참고한 정보의 이름과 페이지 알려줘\n",
    "2. 입력에 틀린 부분이 하나도 없을 경우에만 칭찬 한 문장 해줘\n",
    "3. 다른 미사여구 없이 본론부터 말해줘\n",
    "\n",
    "#정보:\n",
    "{context}\n",
    "\n",
    "#입력:\n",
    "{question}\n",
    "\n",
    "#답:\"\"\"\n",
    ")\n",
    "\n",
    "# 언어모델(LLM) 생성\n",
    "llm = ChatUpstage(model=\"solar-pro\", temperature=0.2)\n",
    "\n",
    "chain = prompt | llm | StrOutputParser()\n",
    "\n",
    "response = chain.invoke({\"context\": retrieved_docs, \"question\": ocr_txt})\n",
    "\n",
    "print(response)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analysis_chunk(input_data):\n",
    "    # LangSmith 시작\n",
    "    logging.langsmith(settings.LANGSMITH_PROJECT_NAME)\n",
    "    db_index_name = settings.PINECONE_INDEX_NAME\n",
    "\n",
    "    pc = Pinecone()\n",
    "    index = pc.Index(db_index_name)\n",
    "    embeddings = UpstageEmbeddings(model=\"embedding-query\")\n",
    "    vectorstore = PineconeVectorStore(index=index, embedding=embeddings)\n",
    "\n",
    "    # 검색기(Retriever) 생성\n",
    "    retriever = vectorstore.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 2})\n",
    "\n",
    "    # input data 청크 나누기\n",
    "    embeddings = UpstageEmbeddings(model=\"embedding-passage\")\n",
    "    text_splitter = SemanticChunker(\n",
    "        embeddings=embeddings,\n",
    "        breakpoint_threshold_type=\"percentile\",\n",
    "        breakpoint_threshold_amount=95,\n",
    "    )\n",
    "    chunks = text_splitter.split_text(input_data)\n",
    "    print(\"input data chunk len: \", len(chunks))\n",
    "\n",
    "    # 청크별 검색\n",
    "    retrieved_docs = []    # Document list\n",
    "    retrieved_ids = []    # RAG id list\n",
    "    for chunk in chunks:\n",
    "        docs = retriever.invoke(chunk)\n",
    "        for doc in docs:\n",
    "            doc_id = doc.id\n",
    "            if not doc_id in retrieved_ids:\n",
    "                retrieved_ids.append(doc_id)\n",
    "                retrieved_docs.append(doc)\n",
    "    print(\"Retrieved docs len: \", len(retrieved_docs))\n",
    "    print(\"Retrieved ids len: \", len(retrieved_ids))\n",
    "\n",
    "    # 프롬프트 생성(Create Prompt)\n",
    "    prompt = PromptTemplate.from_template(\n",
    "        \"\"\"너는 입력을 보고 틀린 부분에 대해서 피드백을 주는 선생님이야\n",
    "    아래 추가 조건과 정보를 참고해서 피드백 해줘\n",
    "    추가 조건\n",
    "    1. 참고한 정보의 이름과 페이지 알려줘\n",
    "    2. 입력에 틀린 부분이 하나도 없을 경우에만 칭찬 한 문장 해줘\n",
    "    3. 다른 미사여구 없이 본론부터 말해줘\n",
    "\n",
    "    #정보:\n",
    "    {context}\n",
    "\n",
    "    #입력:\n",
    "    {question}\n",
    "\n",
    "    #답:\"\"\"\n",
    "    )\n",
    "\n",
    "    # 언어모델(LLM) 생성\n",
    "    llm = ChatUpstage(model=\"solar-pro\", temperature=0.2)\n",
    "\n",
    "    # Chain 생성\n",
    "    chain = prompt | llm | StrOutputParser()\n",
    "\n",
    "    response = chain.invoke({\"context\": retrieved_docs, \"question\": input_data})\n",
    "\n",
    "    results = {\n",
    "        \"rag_id\": retrieved_ids,\n",
    "        \"response\": response\n",
    "    }\n",
    "\n",
    "    return results\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aitech311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
